{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a36f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LevelsFYIScraper.py \n",
    "\n",
    "#Function: to help troubleshoot in case the script input variables are wrong.\n",
    "\n",
    "#Input variables:\n",
    "\n",
    "#s: URL of first page of companies that user wants to scrape\n",
    "#parser: type of parser that user wants to use (ex: html.parser, html5lib, etc.)\n",
    "#offset: used to determine next page of companies that should be recorded (check levels.fyi url on page 2 \n",
    "#                                                                          of your search to see this)\n",
    "#max_offset: the very last page's offset + 1\n",
    "#offset_increment: the increment between page offsets (third page offset - second page offset, \n",
    "#                                                      if only two pages then any number works)\n",
    "#s2: the urls for the companies starting from page 2, determined by taking offsets from vals variable\n",
    "#stack: companies the user wants to record are added to this stack\n",
    "\n",
    "#Output variable:\n",
    "\n",
    "#stack: list of company names that user wants\n",
    "\n",
    "#Instructions:\n",
    "\n",
    "#All arguments except the offsets must be python strings or f-strings.\n",
    "#Offsets must be integers.\n",
    "#When running in terminal, after typing in the name of the .py file, enter in:\n",
    "\n",
    "#The first url to be scraped,\n",
    "#parser type,\n",
    "#offset from second url,\n",
    "#offset from the very last url + 1,\n",
    "#offset increment (third url offset - second url offset, if only two pages then any integer works),\n",
    "#and the second page url with 'offset={vals[i]}' and f in front of the first quotation mark.\n",
    "\n",
    "#Time complexity: O(n*m), where n is the number of urls the user wants to scrape\n",
    "#                         and m is the number of companies per url\n",
    "\n",
    "#Space complexity: O(m), where m is the number of companies per url\n",
    "\n",
    "#################################################################################\n",
    "# Date modified              Modifier             What was modified             #\n",
    "# 06/09/2024                 Eram Kabir           Initial Development           #\n",
    "#################################################################################\n",
    "\n",
    "#libraries\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "#code to run in Jupyter Notebook (if someone does not like scripting, or needs a reference for input variables):\n",
    "\n",
    "#input variable s\n",
    "s = \"https://www.levels.fyi/jobs/location/new-york-usa?locationSlug=united-states&searchText=software+engineer&jobId=99222064430228166\"\n",
    "\n",
    "r = requests.get(s) #get html content from s\n",
    "\n",
    "parser = 'html.parser' #choose type of html parser for BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(r.content, parser) #parse the html content from s with parser\n",
    "\n",
    "stack = [soup.select('h2')[i].next for i in range(len(soup.select('h2')))] #get companies from s\n",
    "\n",
    "offset = 5 #check offset on second page\n",
    "max_offset = 196 #check offset on last page and +1 to it\n",
    "offset_increment = 5 #third page offset - second page offset\n",
    "vals = [str(i) for i in range(offset, max_offset, offset_increment)] #list of offsets (used to construct s2)\n",
    "\n",
    "for i in range(0, len(vals)): #for each url\n",
    "    #input variable s2, made from vals and second url\n",
    "    s2 = f\"https://www.levels.fyi/jobs/location/new-york-usa?locationSlug=united-states&offset={vals[i]}&searchText=software+engineer&jobId=99222064430228166\"\n",
    "    \n",
    "    driver = webdriver.Chrome() #type of driver to use to open website (chrome, in this case)\n",
    "    driver.get(s2) #open website\n",
    "    r = driver.page_source #get html from open website\n",
    "    soup = BeautifulSoup(r, parser) #parse html from open website\n",
    "    for j in range(0, len(soup.select('h2'))):\n",
    "        \n",
    "        #for each company on the open website:\n",
    "        \n",
    "        #if the company is not in the stack already, and is not \"Company Details\" and does not have \"levels.fyi\"\n",
    "        \n",
    "        #add it to the stack\n",
    "        \n",
    "        if soup.select('h2')[j].next not in stack and soup.select('h2')[j].next != \"Company Details\" and \"levels.fyi\" not in soup.select('h2')[j].next:\n",
    "            stack.append(soup.select('h2')[j].next)\n",
    "\n",
    "print(stack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
